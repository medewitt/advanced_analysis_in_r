---
title: "Advanced Data Analysis in R"
subtitle: "Bayesian Modeling in R"
author: "Michael DeWitt"
date: "2018-03-17 (Updated `r Sys.Date()`)"
output:
  beamer_presentation:
    keep_tex: false
    theme: metropolis
    slide_level: 2
    incremental: false
    includes:
      in_header: head.txt
fontsize: 10pt
#classoption: compress
classoption: "aspectratio=169"
bibliography: my_bib.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center"
)
library(tidyverse)
library(broom)
library(here)
set.seed(27)
options(
  tibble.print_max = 3,
  tibble.print_min = 3,
  width = 65
)
theme_set(theme_gray(base_size = 22))
```

# Bayesian Modeling in R

## A Thought Exercise

You are already Bayesian!  

You just didn't know it!

## A Coin

What is the probability a given coin is fair?

## Frequentist

If you didn't answer 100% or 0% you're Bayesian!

## Bayes Theorem

Just derived from identities of probability

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$


# Prior, Likelihood, Posterior Distribution

## Prior- What do we know (or not)?

Prior is _subjective_ and represents a range of potential values 

Specified by a distribution  

### Informative

Restricts range of likely values to small range

More informative adds more "weight" to the prior vs the data  

### Uninformative/ Weak

Wider range of possibilities  

More closely approximates the Maximum likelihood estimates  

## Likelihood - The Data Generating Process

Likelihood is the distribution for the data generating process  

Examples 

- Poisson process -> poisson likelihood function  
- Binomial process -> binomial likelihood function 
- Normal distribution -> normal likelihood function  
- Ordered categorical -> ordered categorical likelihood function

## Posterior - What we make inferences on!

$$P(A|B) \sim Prior * Likelihood$$

```{r relationship-prior, echo=FALSE, out.width="70%"}
example_df <- data.frame(x = seq(0,1, .01)) %>% 
  mutate(prior = dbeta(x, 1, 1)) %>% 
  mutate(likelihood = dbinom(x*100, size = 100, prob = .5)) %>%  
  mutate(posterior = dbeta(x, 50+1, 1 + 50)) %>% 
  gather(measure, value, -x)

example_df %>% 
  ggplot(aes(x, value, color = measure))+
  geom_line()+
    facet_wrap(~measure, scales = "free_y")+
  theme(legend.position = "none")
```

## Strong Priors

Say, I had a stronger prior...

```{r informative-prior, echo=FALSE, out.width="70%"}
example_df <- data.frame(x = seq(0,1, .01)) %>% 
  mutate(prior = dbeta(x, 100, 1)) %>% 
  mutate(likelihood = dbinom(x*100, size = 100, prob = .5)) %>%  
  mutate(posterior = dbeta(x, 50+100, 1 + 50)) %>% 
  gather(measure, value, -x)

example_df %>% 
  ggplot(aes(x, value, color = measure))+
  geom_line()+
    facet_wrap(~measure, scales = "free_y")+
  theme(legend.position = "none")
```


# Bayesian Workflow

## Doing Bayesian Inferences

Bayesian inference and modeling techniques can be applied across the board  

In MLE approaches you often make assumptions without even realising it!  

Bayes requires you to be more deliberate  

## Write Your Model

What is your data generating process?  

> We are estimating the support for a given referendum. Thus our population has a choice, either 1 (support) or 0 (do not support). We do not have any good estimates from previous literature for overall support. 

## Write The DGP in Math

### Likelihood Function

Series of Bernouilli trials -> binomial likelihood function  

$$P(y_i = 1|income, gender_i) \sim logit^{-1}(\mu_i)$$

Where,

$$\mu = normal(\beta_{1}*income + \beta_2*gender + intercept, \sigma)$$

### Prior

Additionally say that we believe that the impact of these two metrics aren't too strong

$$\beta_1 \sim N(0,0.5)$$
$$\beta_1 \sim N(0,0.5)$$



## Simulate Your Data Generating Process

```{r}
n <- 1000
gender <-rep(x = 0:1, length.out = n)
income <- rnorm(n, 0, 1)
mu <- gender * 1.5 + income * 2
y <- rbinom(n,1, prob = plogis(mu))

dat <- data.frame(gender =gender, 
                  income = income, 
                  y = y)
```


## Domain Specific Languages

Implementation of Bayesian Data Analysis can be done manually...

But there exist domain specific languages to handle most cases:

- BUGS    
- JAGS    
- [Stan](https://mc-stan.org/users/)   
- [Hand coded samplers](https://michaeldewittjr.com/dewitt_blog/posts/2019-04-04-speeding-things-up-with-rcpp/)  

## Enter `brms`

`brms` makes Bayesian modeling easy

Compiles traditional R and `lme4` syntax to Stan 

Utilises Hamiltonian Monte Carlo with a No U-Turn Sampler


## Specifying a Model in `brms`

Models can specified in-line or separately using the `bf` function 

```{r}
library(brms)
(model <- bf(y ~ gender + income))
```

## Inspect Priors

The `get_prior` function allows the user to see what priors can be specified  

```{r}
get_prior(model, dat, bernoulli())
```

## Specify Priors

Priors can then be specified using the existing [disribution families](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html)

```{r}
my_priors <- c(
  prior(normal(0, 0.5), class = "b", coef = "gender"),
  prior(normal(0, 0.5), class = "b", coef = "income"))
```


## Model Family

```{r fit-model, cache=TRUE}
fit <- brm(model, my_priors,
           data =  dat, 
           family = bernoulli(), 
           inits = 1000, cores = 2, 
           chains = 2, seed = 1234, refresh = 0)
```

## Posterior Checks

- Convergence  
  - Trace Plots `mcmc_trace`
  - Rhat metrics
  - Effective Sample Size
  
- Posterior Predictive Intervals
  - Was there a good fit between the model and the data
  
## So Let's Check

With trace plots we are looking to ensure that there are not divergent chains

```{r out.width="60%"}
library(tidybayes)
library(bayesplot)
mcmc_trace(as.matrix(fit))
```

## Rhat and Effective Sample Size

Rhat is a measure of chain missing (target < 1.01)

```{r}
summary(fit)
```

## Convergence of Parameters

Here looking for the centeredness of our parameters  

```{r out.width="60%"}
pairs(fit)
```


## Posterior Predictive Checks

Not covered here, but more common when predicting continuous variables

Assess how well your model can predict the data that generated it.

See [this tutorial for details](https://mc-stan.org/bayesplot/)

## Inferences

Now that we have validated our fit, we can make inferences

```{r}
summary(fit)
```

## Inferences on Parameters

Posterior distribution of parameters

```{r out.width="60%"}
mcmc_areas(as.matrix(fit), 
           pars = c("b_gender", "b_income"), 
           prob = .8)
```

## Convergence

If your model takes a long time to fit OR doesn't have good convergence...

- Increase number of iterations 

- Change your `max_treedepth` and `adapt_delta` 

- Add stronger priors 

- Increase the "thinning"  

- (re)-Scale parameters 

- You may have a poorly fitting model  

## Drawbacks of Bayesian Inference

- Not as widely utilised in major publications  

- Computationally intensive 

- Picking a prior  

- Heuristics [exist](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)

## Advantages of Bayesian Analysis

- Takes advantage of expert opinion  

  - Especially helpful with small samples size studies  
  
  - Reduces possibility of wildly odd results  

- Easier communications (more intuitive to discuss probabilities)  

- Studies can build on one another

  - Results from one study can be supplied directly as a prior into a replication or another study

## References

Check out <https://michaeldewittjr.com/resources/> under the _Stan_ tab for more worked examples
